---
title: "Walkthrough - Step 1 and 2 - Import and Process Text"
author: "Evan Canfield"
date: "11/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
The purpose of this notebook is walk through the text processing steps using PDF text generated from Tika

## Data
```{r import_data}
demo_path <- "../data/input_pdfs/a94amj.pdf"

# Import Single Document
demo_raw <- tika_text(demo_path)

# Patterns
patterns_col <- c("remove","comments")
patterns_raw <- read_excel(path = "../data/patterns.xlsx", col_names = patterns_col, )
patterns_raw <- patterns_raw %>% pull(remove)
```

## Process Text Functions
```{r}
# demo_raw
```


```{r}
## Split Text into Character Vector
processing_text <- demo_raw %>%
  str_split(pattern = "\n") %>%
  unlist()

processing_text
```

## Remove Patterns Dataset
#### Split New Lines
The data comes with an Excel spreadsheet filled with specific strings known in the available data that are to be dropped. The patterns have already been imported. Next, any element in the vecotr which contains a newline, is split into two (or more), separate vectors. 
```{r}
patterns <- str_split(patterns_raw, "\r\n") %>% unlist()
```

#### Emply and Missing Elements
Drop any empty vector elements ("") and NA elements,
```{r}
patterns <- patterns[patterns!=""]
patterns <- patterns[!is.na(patterns)]

inspect_vector(patterns)
```


#### Processing
We need to apply the same processing to the removal patterns that we have already applied to the text data. 
```{r}
patterns <- patterns %>% 
  str_squish()

inspect_vector(patterns)
```


# Build_TrainSet_Hypho_Class.py
The following section will walk through the data processing steps in file **Build_TrainSet_Hypho_Class.py**.

## Function: clean_and_convert_to_sentence
The following function is used to create the Cleaned Text via JB, per **Build_TrainSet_Hypho_Class.py**.
### Remove Any Text After References / Bibiography
We will include  multiple test strings string to ensure this method works, as a References of Bibliography section may not be part of each paper.
```{r}
# test_doc <- demo_001[1:10]
# test_str_1 <- "^resources may be seen as a source of$"
# test_str_2 <- "^lying assumption is that human resources$"
# 
# ## The ^ $ for a regex complete match
# ref_bib_key <- c("^references$", "^bibliography$", test_str_1, test_str_2)
# 
# # Create Or Regex Search for All Possible Values
# test_regex <- paste(ref_bib_key, collapse = "|")
# 
# # Return Logical Vector if string was matched
# ref_bib_logical <- str_detect(test_doc, test_regex)
# 
# # Return Latest Index Where True
# index <- max(which(ref_bib_logical == TRUE))
# 
# test_doc_trim <- test_doc[1:index-1]
# test_doc_trim
```

### General Case: Remove References / Bibliograph Section
The above example uses known list values to test that lines were correctly dropped. The following will be a general case, only using the expected terms in the final case.
```{r}
input <- demo_001

## The ^ $ for a regex complete match
section_key <- c("References", "Bibliography",
                 "REFERENCES", "BIBIOGRAPHY")

regex_section <- gen_regex(input_vector = section_key, match = "exact")

# Return Logical Vector if string was matched
logical_section <- str_detect(input, regex_section)

# Return Latest Index Where True

if (any(logical_section)){
  index <- max(which(logical_section == TRUE))
  output <- input[1:index-1]
}

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_001 <- output
```

### Drop Elements in Text Which Match Removal Patterns
With our patterns dataset now processed, we will drop any line in our journal text data that matches a line in the patterns vector.

```{r}
input <- clean_text_JB_001
  
output <- input[!input %in% patterns]

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_002 <- output
```

### Drop Elements With Only Number or Symbols
```{r}
input <- clean_text_JB_002

regex_letters <- '[a-zA-Z]'

output <- remove_if_detect(
  input_vector = input, 
  regex_letters,
  logical_method = "inverse"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_003 <- output
```

### Drop Elements With Length of Only 1
```{r}
input <- clean_text_JB_003

logical_length <- nchar(input) > 1

output <- input[logical_length]

# Drop NA
output <- output[!is.na(output)]

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_004 <- output
```

### Drop Element Which Starts with Month
The month must be ALL CAPS
```{r}
input <- clean_text_JB_004

output <- remove_if_detect(
  input_vector = input, 
  remove_vector = toupper(month.name),
  location = "start"
  )

output <- output[!is.na(output)]

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_005 <- output
```

### Concatenate Adjacent Elements If Initial Element Ends With Hyphen
```{r}
input <- clean_text_JB_005

output <- concat_hypen_vector(input)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_006 <- output
```

### Remove Lines Which Contain Terms Related to Downloading 
```{r}
input <- clean_text_JB_006

elements_download <- c('This content downloaded','http','jsto','DOI','doi')

output <- remove_if_detect(
  input_vector = input, 
  remove_vector = elements_download,
  location = "any"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_007 <- output
```

### Remove Lines Containing IP Address
```{r}
input <- clean_text_JB_007

regex_ip <- "(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})\\.(?:[\\d]{1,3})"

output <- remove_if_detect(
  input_vector = input, 
  regex = regex_ip,
  location = "any"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_008 <- output
```

### Remove Text In Paranthesis
```{r}
input <- clean_text_JB_008

# Define Term to Identify Line Splits
line_split_indicator <- " -LINESPLIT-"

# Concatenate All Vector Elements, Separated By Line Split
output <- str_c(input, collapse = line_split_indicator)

# Define Regex Expression to Identify Parenthesis and Contents Within
regex_parens <- "\\(([^()]+)\\)"

# Remove Content Within Parenthesis
output <- str_remove_all(string = output, pattern = regex_parens)

# Split Single String Back into Character Vector
output <- str_split(output, pattern = line_split_indicator) %>% unlist()

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_009 <- output
```

### Drop Empty Vectors
Drop any empty vector elements ("") and NA elements,
```{r}
input <- clean_text_JB_009

output <- input[input!=""]
output <- output[!is.na(output)]

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_010 <- output
```

### Drop Elements With Only Number or Symbols - Second Time
```{r}
input <- clean_text_JB_010

regex_letters <- '[a-zA-Z]'

output <- remove_if_detect(
  input_vector = input, 
  regex_letters,
  logical_method = "inverse"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_011 <- output
```


### Tokenize Sentences
```{r}
input <- clean_text_JB_011

# Concatenate All Vector Elements, Separated By Line Split
output <- str_c(input, collapse = " ")
output <- tokenize_sentences(output, strip_punct = FALSE) %>% unlist()

# Replace Double Spaces
output<- str_replace_all(string = output, pattern = "  ", replacement = " ")

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_012 <- output
```

### Remove Lines Which Contain Terms Related to Downloading - Second Time
```{r}
input <- clean_text_JB_012

elements_download <- c('This content downloaded','http','jsto','DOI','doi')

output <- remove_if_detect(
  input_vector = input, 
  remove_vector = elements_download,
  location = "any"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_013 <- output
```

### Drop Elements With Only Number or Symbols - Third Time
This is not in the original python code. This third pass is recommended due to the sentence tokenizer step occuring previously.
```{r}
input <- clean_text_JB_013

regex_letters <- '[a-zA-Z]'

output <- remove_if_detect(
  input_vector = input, 
  regex_letters,
  logical_method = "inverse"
  )

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_013.1 <- output
```

### Replace Cases of Hypthesis with Hypo 
#### Initialize Regex Expressions
```{r}
regex_hypo_vec <-c('h[0-9]{1,3}[a-zA-Z]\\:',
                   'H[0-9]{1,3}[a-zA-Z]\\:',
                   'h[0-9]{1,3}[a-zA-Z]\\.',
                   'H[0-9]{1,3}[a-zA-Z]\\.',
                   'h[0-9]{1,3}[a-zA-Z]',
                   'H[0-9]{1,3}[a-zA-Z]',
                   'hypothesis [0-9]{1,3}[a-zA-Z]\\:',
                   'Hypothesis [0-9]{1,3}[a-zA-Z]\\:',
                   'hypothesis [0-9]{1,3}[a-zA-Z]\\.',
                   'Hypothesis [0-9]{1,3}[a-zA-Z]\\.',
                   'hypothesis [0-9]{1,3}[a-zA-Z]',
                   'Hypothesis [0-9]{1,3}[a-zA-Z]',
                   'h[0-9]{1,3}\\:',
                   'H[0-9]{1,3}\\:',
                   'h[0-9]{1,3}\\.',
                   'H[0-9]{1,3}\\.',
                   'h[0-9]{1,3}',
                   'H[0-9]{1,3}',
                   'hypothesis [0-9]{1,3}\\:',
                   'Hypothesis [0-9]{1,3}\\:',
                   'hypothesis [0-9]{1,3}\\.',
                   'Hypothesis [0-9]{1,3}\\.',
                   'hypothesis [0-9]{1,3}',
                   'Hypothesis [0-9]{1,3}') 

regex_hypo_str <- gen_regex(regex_hypo_vec, match = "partial")

# Regex - Numbers in a String
regex_return_num <- "(\\d)+"

```

#### Extract Detected Hypothesis
##### Determine Which Indexes Contain Hypothesis
```{r}
input <- clean_text_JB_013.1

# Check Which Strings Contain a Hypothesis
extract_hypo <- str_extract(string = input, pattern = regex_hypo_str)

index_hypo <- 86
extract_hypo[index_hypo]

```

##### Test 
```{r}
input <- clean_text_JB_013.1

input_demo <- input[index_hypo]

input_demo

extract_demo <- str_extract(input_demo, regex_hypo_str)
extract_demo_num <- str_extract(extract_demo, regex_return_num)
extract_demo_replace <- paste0("<split>Hypo ", extract_demo_num)
output_demo  <- str_replace_all(string = input_demo, pattern = regex_hypo_str, replacement = extract_demo_replace)
input_demo
output_demo

paste0("<split>Hypo ", extract_demo_num)
str_replace(string = input_demo, pattern = regex_hypo_str, replacement = regex_return_num)
```

#### Final Transformation
```{r}
input <- clean_text_JB_013.1

output <- replace_hypo(input)
output <- unname(output)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_014 <- output
```


```{r}
output[index_hypo]
```


### Replace Colon Colon (: :) Instances
```{r}
input <- clean_text_JB_014

output <- str_replace_all(input, pattern = ": :", replacement = ":")

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_015 <- output
```


```{r}
output[index_hypo]
```

### Remove Extra Whitespace
```{r}
input <- clean_text_JB_015
output <- str_squish(string = input)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_016 <- output
```

```{r}
output[index_hypo]
```


### Replace Colon Period (: .) Instances
```{r}
input <- clean_text_JB_016

## Double Stroke Is To Escpae The Period as a Metacharacter
output <- str_replace_all(input, pattern = ": \\.", replacement = ":")

vector_length_comp(input, output)
inspect_vector(output)

clean_text_JB_017 <- output
```


```{r}
output[index_hypo]
```

## Function: rm_breaks
The following function is used to create the Cleaned Text via VF, per **Build_TrainSet_Hypho_Class.py**.

### Convert to Lower Case
```{r}
input <- demo_001

output <- input %>% tolower()

vector_length_comp(input, output)
inspect_vector(output)


clean_text_VF_001 <- output
```

### Remove Commas
```{r}
input <- clean_text_VF_001

output <- str_remove_all(input, ",")

vector_length_comp(input, output)
inspect_vector(output)


clean_text_VF_002 <- output
```

### Remove DOIs
```{r}
input <- clean_text_VF_002 

regex_doi_1 <- '\\d+\\.\\d+/\\w+'
regex_doi_2 <- 'doi:*'

output <- str_remove_all(string = input, pattern = regex_doi_1)
output <- str_remove_all(string = output, pattern = regex_doi_2)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_003 <- output

```

### Replace / Recode Hypotheses
```{r}
input <- clean_text_VF_003

regex_hypo_2 <- "hypothesis (?=\\d+)"

output <- str_replace_all(string = input, pattern = "hypotheses", replacement = "hypothesis")
output <- str_replace_all(string = output, pattern = regex_doi_2, replacement = "h")

vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_004 <- output

```

### Remove Numbers That Do Not Have a Character Directly Preceding
```{r}
input <- clean_text_VF_004

regex_alphanum_1 <- "\\W+\\d+"
regex_alphanum_2 <- "\\d{2,4}"

output <- str_remove_all(string = input, regex_alphanum_1)
output <- str_remove_all(string = output, regex_alphanum_2)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_005 <- output
```

### Remove JSTOR Link
```{r}
input <- clean_text_VF_005

regex_jstor_link <- 'https?://.+'
regex_jstor_2 <- '\\.{2,}|:'
regex_jstor_3 <- 'this\\scontent.+'

output <- str_replace_all(string = input, pattern = regex_jstor_link, replacement = "jstor. ")
output <- str_remove_all(string = output, pattern = regex_jstor_2) 
output <- str_remove_all(string = output, pattern = regex_jstor_3) 
output <- str_remove_all(string = output, pattern = '.*jstor.*') 


vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_006 <- output
```


### Remove Word Interuptions
```{r}
input <- clean_text_VF_006

regex_interuptions <- '-\\s*\\n\\s*'

output <- str_remove_all(string = input, regex_interuptions)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_007 <- output

```

### Remove Line Breaks
```{r}
input <- clean_text_VF_007

regex_line_break_1 <- "\\r\\n"
regex_line_break_2 <- "\\n"

output <- str_remove_all(string = input, regex_line_break_1)
output <- str_remove_all(string = output, regex_line_break_2)

vector_length_comp(input, output)
inspect_vector(output)

clean_text_VF_008 <- output
```

