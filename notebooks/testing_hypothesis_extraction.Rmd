---
title: "Entity Extraction Performance Evaluation - R & Python"
author: "Evan Canfield"
date: "12/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose
The purpose of this notebook is to test how to implement the Entity Extraction model 

# Import
## Libraries
```{r import_libraries}
  if (!require(pacman)) {install.packages('pacman')}
  p_load(
    dplyr,
    readxl,
    reticulate,
    stringr,
    tokenizers,
    tidyr
  )
```

## Source Files
The following imports functions defined in the sourced R scripts.
```{r}
# Import All Scripts
script_path <- "../R/"
file_paths <- list.files(recursive = TRUE, 
                         path = script_path, pattern = ".R", 
                         full.names = TRUE)

for (file in file_paths){
  source(file)
}
```
## fastText Model
```{r}
## Load Model
path_ft_model <- "./../models/fasttext_model.bin"
ft_model <- fastTextR::ft_load(path_ft_model)
```

## Data
```{r import_data}
# PDF Input
## Define Path
pdf_path <- "./../data/acadmic_papers_pdf_sample/jv04amj.pdf"

pdf_txt_raw <- pdf_to_text(pdf_path)


# Error Word Splits
df_word_split_error <- read.csv(file ="./../data/next_line_split_error.csv", 
                                stringsAsFactors = FALSE)
word_split_error <- df_word_split_error %>% select(word) %>% pull
```

# Process Data
## Preprocess Text Steps
```{r}
# Process Text
text_processed <- process_text(input_text = pdf_txt_raw)

input_text <- text_processed
```

## Processing - Hypothesis Exrtaction
```{r}
  # Concatenate All Vector Elements, Separated By Line Split
  processing_text <- str_c(input_text, collapse = " ")
  processing_text <- tokenize_sentences(processing_text,
                                        strip_punct = FALSE) %>% unlist()

  # Replace Double Spaces
  processing_text <- str_replace_all(string = processing_text,
                            pattern = "  ",
                            replacement = " ")

  # Normalize Text ------------------------------------------------------------
  processing_text <- tolower(processing_text)
```

```{r}
# Regex - Between Hypo and Colon
regex_hypo_marker <- "<split>hypo (.*?):"

# Hypothesis Extraction -----------------------------------------------------
  # Identify Lines with Hypothesis Pattern
  h_match <- processing_text %>% str_match(regex_hypo_marker)
  # Extract Hypotheses Number
  h_match_num <- h_match[,2]
  
  # Identify Unique Hypothesis Numbers
  h_match_num_unq <- unique(h_match_num)
  
  # Drop NA
  h_match_num_unq <- h_match_num_unq[!is.na(h_match_num_unq)]
  
  # Determine Vector Index of Initial Hypothesis Statements
  h_initial <- c()
  for (i in h_match_num_unq){
    intial_idx <- tapply(seq_along(h_match_num),
                         h_match_num,
                         min)[i]
    h_initial <- c(h_initial, intial_idx)
  }
  
  # Reduce Text to Only Initial Hypothesis Instances
  h_statements <- processing_text[h_initial]

```

```{r}
  # Split Statements On Indicator (Defined in Processing) ---------------------
  ## Define
  split_indicator <- "<split>"

  ## Split on Indicator
  h_statements <- str_split(string = h_statements,
                                     pattern = split_indicator) %>%
    unlist()

  ## Detect Statements Which Contain "Hypo"
  logical_hypothesis_2 <- str_detect(h_statements, "hypo")

  ## Drop Statements that Do Not Include "Hypo"
  h_statements <- h_statements[logical_hypothesis_2]
  
  h_statements

```

```{r}
h_statements

detect_string <- ".*: "

h_statements %>% 
  str_extract("hypo (.*?):") %>% 
  str_remove_all("hypo ") %>% 
  str_remove_all(":") %>% 
  as.integer()



h_number <- h_statements %>% 
  str_extract(detect_string) %>% 
  str_remove_all("hypo ") %>% 
  str_remove_all(": ") %>% 
  as.integer()

h_number

output <- vector(mode = "logical", length = length(h_number))
tracker <- vector(mode = "integer", length = length(h_number))
tracker
for (i in seq_along(h_number)) {
  num <- h_number[i]

  if (is.na(num)){
    
    output[i] = FALSE
    tracker[i] <- -1
    
  } else if (num %in% tracker) {
    
    output[i] = FALSE
    tracker[i] <- -1
    
  } else {
    
    output[i] = TRUE
    tracker[i] <- num

  }
}

h_statements <- h_statements[output]
```

```{r}

  for (i in seq_along(word_split_error)) {
    word_split <- word_split_error[i]
    word_fix <- str_replace_all(string = word_split, 
                                  pattern = " ",
                                  replacement = "")
    
    h_statements <- h_statements %>% 
      str_replace_all(pattern = word_split, replacement = word_fix)
  }

h_statements
```



```{r}
  # Drop ~Hypo #:~
  h_statements <- gsub(".*: ","",h_statements)

  # Drop Empty Strings
  h_statements <- h_statements[h_statements != ""]
  
  
  h_statements
```
```{r}
# Fasttext Verification
hypothesis_pred <- fastTextR::ft_predict(ft_model, 
                                         newdata = h_statements, 
                                         rval = "dense") %>% 
        as.data.frame()

col_names <- names(hypothesis_pred)

if ("__label__0" %in% col_names) {
  response <- hypothesis_pred %>% 
    mutate(
        Response = if_else(.[[1]] > .[[2]], FALSE, TRUE)
    ) %>% pull(Response)
  
  h_statements <- h_statements[response]
}

h_statements
```


```{r}
# Convert Text
  h_statements <- str_to_sentence(h_statements, locale = "en")
```


```{r}
  # Create Dataframe with Hypothesis Number and Hypothesis
  df_hypothesis <- as.data.frame(h_statements,
                                 stringsAsFactors = FALSE)
```


```{r}
df_hypothesis
```
